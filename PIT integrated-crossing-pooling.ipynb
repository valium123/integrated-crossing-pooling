{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\r\n",
    "import numpy as np\r\n",
    "import paddle.optimizer as opt\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "\r\n",
    "from paddle.nn.initializer import TruncatedNormal, Constant, Assign\r\n",
    "trunc_normal_ = TruncatedNormal(std=.02)\r\n",
    "zeros_ = Constant(value=0.)\r\n",
    "ones_ = Constant(value=1.)\r\n",
    "class Identity(nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(Identity, self).__init__()\r\n",
    "\r\n",
    "    def forward(self, input):\r\n",
    "        return input\r\n",
    "\r\n",
    "\r\n",
    "class Mlp(nn.Layer):\r\n",
    "    def __init__(self,\r\n",
    "                 in_features,\r\n",
    "                 hidden_features=None,\r\n",
    "                 out_features=None,\r\n",
    "                 act_layer=nn.GELU,\r\n",
    "                 drop=0.):\r\n",
    "        super().__init__()\r\n",
    "        out_features = out_features or in_features\r\n",
    "        hidden_features = hidden_features or in_features\r\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\r\n",
    "        self.act = act_layer()\r\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\r\n",
    "        self.drop = nn.Dropout(drop)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.fc1(x)\r\n",
    "        x = self.act(x)\r\n",
    "        x = self.drop(x)\r\n",
    "        x = self.fc2(x)\r\n",
    "        x = self.drop(x)\r\n",
    "        return x\r\n",
    "def drop_path(x, drop_prob=0., training=False):\r\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\r\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\r\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ...\r\n",
    "    \"\"\"\r\n",
    "    if drop_prob == 0. or not training:\r\n",
    "        return x\r\n",
    "    keep_prob = paddle.to_tensor(1 - drop_prob)\r\n",
    "    shape = (paddle.shape(x)[0], ) + (1, ) * (x.ndim - 1)\r\n",
    "    random_tensor = keep_prob + paddle.rand(shape, dtype=x.dtype)\r\n",
    "    random_tensor = paddle.floor(random_tensor)  # binarize\r\n",
    "    output = x.divide(keep_prob) * random_tensor\r\n",
    "    return output\r\n",
    "\r\n",
    "\r\n",
    "class DropPath(nn.Layer):\r\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, drop_prob=None):\r\n",
    "        super(DropPath, self).__init__()\r\n",
    "        self.drop_prob = drop_prob\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return drop_path(x, self.drop_prob, self.training)\r\n",
    "\r\n",
    "class Attention(nn.Layer):\r\n",
    "    def __init__(self,\r\n",
    "                 dim,\r\n",
    "                 num_heads=8,\r\n",
    "                 qkv_bias=False,\r\n",
    "                 qk_scale=None,\r\n",
    "                 attn_drop=0.,\r\n",
    "                 proj_drop=0.):\r\n",
    "        super().__init__()\r\n",
    "        self.num_heads = num_heads\r\n",
    "        head_dim = dim // num_heads\r\n",
    "        self.scale = qk_scale or head_dim**-0.5\r\n",
    "\r\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\r\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\r\n",
    "        self.proj = nn.Linear(dim, dim)\r\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # B= paddle.shape(x)[0]\r\n",
    "        N, C = x.shape[1:]\r\n",
    "        qkv = self.qkv(x).reshape((-1, N, 3, self.num_heads, C //\r\n",
    "                                   self.num_heads)).transpose((2, 0, 3, 1, 4))\r\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\r\n",
    "\r\n",
    "        attn = (q.matmul(k.transpose((0, 1, 3, 2)))) * self.scale\r\n",
    "        attn = nn.functional.softmax(attn, axis=-1)\r\n",
    "        attn = self.attn_drop(attn)\r\n",
    "\r\n",
    "        x = (attn.matmul(v)).transpose((0, 2, 1, 3)).reshape((-1, N, C))\r\n",
    "        x = self.proj(x)\r\n",
    "        x = self.proj_drop(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "class Block(nn.Layer):\r\n",
    "    def __init__(self,\r\n",
    "                 dim,\r\n",
    "                 num_heads,\r\n",
    "                 mlp_ratio=4.,\r\n",
    "                 qkv_bias=False,\r\n",
    "                 qk_scale=None,\r\n",
    "                 drop=0.,\r\n",
    "                 attn_drop=0.,\r\n",
    "                 drop_path=0.,\r\n",
    "                 act_layer=nn.GELU,\r\n",
    "                 norm_layer='nn.LayerNorm',\r\n",
    "                 epsilon=1e-5):\r\n",
    "        super().__init__()\r\n",
    "        self.norm1 = eval(norm_layer)(dim, epsilon=epsilon)\r\n",
    "        self.attn = Attention(\r\n",
    "            dim,\r\n",
    "            num_heads=num_heads,\r\n",
    "            qkv_bias=qkv_bias,\r\n",
    "            qk_scale=qk_scale,\r\n",
    "            attn_drop=attn_drop,\r\n",
    "            proj_drop=drop)\r\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\r\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else Identity()\r\n",
    "        self.norm2 = eval(norm_layer)(dim, epsilon=epsilon)\r\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\r\n",
    "        self.mlp = Mlp(in_features=dim,\r\n",
    "                       hidden_features=mlp_hidden_dim,\r\n",
    "                       act_layer=act_layer,\r\n",
    "                       drop=drop)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\r\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\r\n",
    "        return x\r\n",
    "\r\n",
    "class Transformer(nn.Layer):\r\n",
    "    def __init__(self, base_dim, depth, heads, mlp_ratio,\r\n",
    "                 drop_rate=.0, attn_drop_rate=.0, drop_path_prob=None):\r\n",
    "        super(Transformer, self).__init__()\r\n",
    "        self.layers = nn.LayerList([])\r\n",
    "        embed_dim = base_dim * heads\r\n",
    "\r\n",
    "        if drop_path_prob is None:\r\n",
    "            drop_path_prob = [0.0 for _ in range(depth)]\r\n",
    "\r\n",
    "        self.blocks = nn.LayerList([\r\n",
    "            Block(\r\n",
    "                dim=embed_dim,\r\n",
    "                num_heads=heads,\r\n",
    "                mlp_ratio=mlp_ratio,\r\n",
    "                qkv_bias=True,\r\n",
    "                drop=drop_rate,\r\n",
    "                attn_drop=attn_drop_rate,\r\n",
    "                drop_path=drop_path_prob[i],\r\n",
    "                norm_layer='nn.LayerNorm',\r\n",
    "                epsilon=1e-6,\r\n",
    "            )\r\n",
    "            for i in range(depth)])\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        n, c, h, w = x.shape\r\n",
    "        x = x.transpose((0, 2, 3, 1))\r\n",
    "        x = paddle.flatten(x, start_axis=1, stop_axis=2)\r\n",
    "\r\n",
    "        for blk in self.blocks:\r\n",
    "            x = blk(x)\r\n",
    "\r\n",
    "        x = x.transpose((0, 2, 1))\r\n",
    "        x = x.reshape((n, c, h, w))\r\n",
    "        return x\r\n",
    "\r\n",
    "class conv_head_pooling(nn.Layer):\r\n",
    "    def __init__(self, in_feature, out_feature, stride,\r\n",
    "                 padding_mode='zeros'):\r\n",
    "        super(conv_head_pooling, self).__init__()\r\n",
    "\r\n",
    "        self.conv = nn.Conv2D(in_feature, out_feature, kernel_size=stride + 1,\r\n",
    "                              padding=stride // 2, stride=stride,\r\n",
    "                              padding_mode=padding_mode, groups=in_feature)\r\n",
    "        self.fc = nn.Linear(in_feature, out_feature)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "\r\n",
    "        x = self.conv(x)\r\n",
    "\r\n",
    "        return x\r\n",
    "\r\n",
    "\r\n",
    "class conv_embedding(nn.Layer):\r\n",
    "    def __init__(self, in_channels, out_channels, patch_size,\r\n",
    "                 stride, padding):\r\n",
    "        super(conv_embedding, self).__init__()\r\n",
    "        self.conv = nn.Conv2D(in_channels, out_channels, kernel_size=patch_size,\r\n",
    "                              stride=stride, padding=padding, bias_attr=True)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv(x)\r\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 2304]\n",
      "[16, 10]\n"
     ]
    }
   ],
   "source": [
    "class PoolingTransformer(nn.Layer):\r\n",
    "    def __init__(self, image_size, patch_size, stride, base_dims, depth, heads,\r\n",
    "                 mlp_ratio, in_chans=3, attn_drop_rate=.0, drop_rate=.0,\r\n",
    "                 drop_path_rate=.0, class_dim=10):\r\n",
    "        super(PoolingTransformer, self).__init__()\r\n",
    "        self.pool1=paddle.nn.MaxPool2D([7,1],1)\r\n",
    "        self.pool2=paddle.nn.AvgPool2D([1,7],1)\r\n",
    "        self.pool3=paddle.nn.MaxPool2D([1,7],1)\r\n",
    "        self.pool4=paddle.nn.AvgPool2D([7,1],1)\r\n",
    "        self.pool5=paddle.nn.AvgPool2D([7,1],1)\r\n",
    "        self.pool6=paddle.nn.MaxPool2D([1,7],1)\r\n",
    "        self.pool7=paddle.nn.AvgPool2D([1,7],1)\r\n",
    "        self.pool8=paddle.nn.MaxPool2D([7,1],1)\r\n",
    "        total_block = sum(depth)\r\n",
    "        padding = 0\r\n",
    "        block_idx = 0\r\n",
    "\r\n",
    "        width = math.floor(\r\n",
    "            (image_size + 2 * padding - patch_size) / stride + 1)\r\n",
    "\r\n",
    "        self.base_dims = base_dims\r\n",
    "        self.heads = heads\r\n",
    "        self.class_dim = class_dim\r\n",
    "\r\n",
    "        self.patch_size = patch_size\r\n",
    "\r\n",
    "        self.pos_embed = self.create_parameter(\r\n",
    "            shape=(1, base_dims[0] * heads[0], width, width),\r\n",
    "            default_initializer=Assign(\r\n",
    "                paddle.randn((1, base_dims[0] * heads[0], width, width))\r\n",
    "            ))\r\n",
    "        self.add_parameter(\"pos_embed\", self.pos_embed)\r\n",
    "\r\n",
    "        self.patch_embed = conv_embedding(in_chans, base_dims[0] * heads[0],\r\n",
    "                                          patch_size, stride, padding)\r\n",
    "\r\n",
    "        self.cls_token = self.create_parameter(\r\n",
    "            shape=(1, 1, base_dims[0] * heads[0]),\r\n",
    "            default_initializer=Assign(\r\n",
    "                paddle.randn((1, 1, base_dims[0] * heads[0]))\r\n",
    "            ))\r\n",
    "        self.add_parameter(\"cls_token\", self.cls_token)\r\n",
    "\r\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\r\n",
    "\r\n",
    "        self.transformers = nn.LayerList([])\r\n",
    "        self.pools = nn.LayerList([])\r\n",
    "\r\n",
    "        for stage in range(len(depth)):\r\n",
    "            drop_path_prob = [drop_path_rate * i / total_block\r\n",
    "                              for i in range(block_idx, block_idx + depth[stage])]\r\n",
    "            block_idx += depth[stage]\r\n",
    "\r\n",
    "            self.transformers.append(\r\n",
    "                Transformer(base_dims[stage], depth[stage], heads[stage],\r\n",
    "                            mlp_ratio, drop_rate, attn_drop_rate, drop_path_prob)\r\n",
    "            )\r\n",
    "            if stage < len(heads) - 1:\r\n",
    "                self.pools.append(\r\n",
    "                    conv_head_pooling(base_dims[stage] * heads[stage],\r\n",
    "                                      base_dims[stage + 1] * heads[stage + 1],\r\n",
    "                                      stride=2\r\n",
    "                                      )\r\n",
    "                )\r\n",
    "\r\n",
    "        self.norm = nn.LayerNorm(base_dims[-1] * heads[-1]*4, epsilon=1e-6)\r\n",
    "        self.embed_dim = base_dims[-1] * heads[-1]\r\n",
    "\r\n",
    "        # Classifier head\r\n",
    "        if class_dim > 0:\r\n",
    "            self.head1 = nn.Linear(base_dims[-1] * heads[-1]*4, class_dim)\r\n",
    "        trunc_normal_(self.pos_embed)\r\n",
    "        self.apply(self._init_weights)\r\n",
    "\r\n",
    "    def _init_weights(self, m):\r\n",
    "        if isinstance(m, nn.LayerNorm):\r\n",
    "            zeros_(m.bias)\r\n",
    "            ones_(m.weight)\r\n",
    "\r\n",
    "    def forward_features(self, x):\r\n",
    "        x = self.patch_embed(x)\r\n",
    "\r\n",
    "        pos_embed = self.pos_embed\r\n",
    "        x = self.pos_drop(x + pos_embed)\r\n",
    "\r\n",
    "        for stage in range(len(self.pools)):\r\n",
    "            x = self.transformers[stage](x)\r\n",
    "            x = self.pools[stage](x)\r\n",
    "        x = self.transformers[len(\r\n",
    "            self.transformers)-1](x)\r\n",
    "        x1=self.pool2(self.pool1(x)).flatten(1)\r\n",
    "        x2=self.pool4(self.pool3(x)).flatten(1)\r\n",
    "        x3=self.pool6(self.pool5(x)).flatten(1)\r\n",
    "        x4=self.pool8(self.pool7(x)).flatten(1)\r\n",
    "        x=paddle.concat(x=[x1, x2,x3,x4], axis=1, name=None)\r\n",
    "        x = self.norm(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.forward_features(x)\r\n",
    "\r\n",
    "        if self.class_dim > 0:\r\n",
    "            x = self.head1(x)\r\n",
    "\r\n",
    "        return x\r\n",
    "\r\n",
    "def pit_s(**kwargs):\r\n",
    "    model = PoolingTransformer(\r\n",
    "        image_size=224,\r\n",
    "        patch_size=16,\r\n",
    "        stride=8,\r\n",
    "        base_dims=[48, 48, 48],\r\n",
    "        depth=[2, 6, 4],\r\n",
    "        heads=[3, 6, 12],\r\n",
    "        mlp_ratio=4,\r\n",
    "        **kwargs\r\n",
    "    )\r\n",
    "    return model\r\n",
    "\r\n",
    "model = pit_s()\r\n",
    "out = model(paddle.randn((1, 3, 224, 224)))\r\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 2304]\n",
      "-------------------------------------------------------------------------------\n",
      "   Layer (type)         Input Shape          Output Shape         Param #    \n",
      "===============================================================================\n",
      "     Conv2D-10      [[16, 3, 224, 224]]   [16, 144, 27, 27]       110,736    \n",
      " conv_embedding-4   [[16, 3, 224, 224]]   [16, 144, 27, 27]          0       \n",
      "    Dropout-112     [[16, 144, 27, 27]]   [16, 144, 27, 27]          0       \n",
      "   LayerNorm-76       [[16, 729, 144]]      [16, 729, 144]          288      \n",
      "    Linear-154        [[16, 729, 144]]      [16, 729, 432]        62,640     \n",
      "    Dropout-113     [[16, 3, 729, 729]]   [16, 3, 729, 729]          0       \n",
      "    Linear-155        [[16, 729, 144]]      [16, 729, 144]        20,880     \n",
      "    Dropout-114       [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "   Attention-37       [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "    Identity-37       [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "   LayerNorm-77       [[16, 729, 144]]      [16, 729, 144]          288      \n",
      "    Linear-156        [[16, 729, 144]]      [16, 729, 576]        83,520     \n",
      "      GELU-37         [[16, 729, 576]]      [16, 729, 576]           0       \n",
      "    Dropout-115       [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "    Linear-157        [[16, 729, 576]]      [16, 729, 144]        83,088     \n",
      "      Mlp-37          [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "     Block-37         [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "   LayerNorm-78       [[16, 729, 144]]      [16, 729, 144]          288      \n",
      "    Linear-158        [[16, 729, 144]]      [16, 729, 432]        62,640     \n",
      "    Dropout-116     [[16, 3, 729, 729]]   [16, 3, 729, 729]          0       \n",
      "    Linear-159        [[16, 729, 144]]      [16, 729, 144]        20,880     \n",
      "    Dropout-117       [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "   Attention-38       [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "    Identity-38       [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "   LayerNorm-79       [[16, 729, 144]]      [16, 729, 144]          288      \n",
      "    Linear-160        [[16, 729, 144]]      [16, 729, 576]        83,520     \n",
      "      GELU-38         [[16, 729, 576]]      [16, 729, 576]           0       \n",
      "    Dropout-118       [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "    Linear-161        [[16, 729, 576]]      [16, 729, 144]        83,088     \n",
      "      Mlp-38          [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "     Block-38         [[16, 729, 144]]      [16, 729, 144]           0       \n",
      "  Transformer-10    [[16, 144, 27, 27]]   [16, 144, 27, 27]          0       \n",
      "     Conv2D-11      [[16, 144, 27, 27]]   [16, 288, 14, 14]        2,880     \n",
      "conv_head_pooling-7 [[16, 144, 27, 27]]   [16, 288, 14, 14]          0       \n",
      "   LayerNorm-80       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-163        [[16, 196, 288]]      [16, 196, 864]        249,696    \n",
      "    Dropout-119     [[16, 6, 196, 196]]   [16, 6, 196, 196]          0       \n",
      "    Linear-164        [[16, 196, 288]]      [16, 196, 288]        83,232     \n",
      "    Dropout-120       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   Attention-39       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Identity-39       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   LayerNorm-81       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-165        [[16, 196, 288]]     [16, 196, 1152]        332,928    \n",
      "      GELU-39        [[16, 196, 1152]]     [16, 196, 1152]           0       \n",
      "    Dropout-121       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Linear-166       [[16, 196, 1152]]      [16, 196, 288]        332,064    \n",
      "      Mlp-39          [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "     Block-39         [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   LayerNorm-82       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-167        [[16, 196, 288]]      [16, 196, 864]        249,696    \n",
      "    Dropout-122     [[16, 6, 196, 196]]   [16, 6, 196, 196]          0       \n",
      "    Linear-168        [[16, 196, 288]]      [16, 196, 288]        83,232     \n",
      "    Dropout-123       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   Attention-40       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Identity-40       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   LayerNorm-83       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-169        [[16, 196, 288]]     [16, 196, 1152]        332,928    \n",
      "      GELU-40        [[16, 196, 1152]]     [16, 196, 1152]           0       \n",
      "    Dropout-124       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Linear-170       [[16, 196, 1152]]      [16, 196, 288]        332,064    \n",
      "      Mlp-40          [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "     Block-40         [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   LayerNorm-84       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-171        [[16, 196, 288]]      [16, 196, 864]        249,696    \n",
      "    Dropout-125     [[16, 6, 196, 196]]   [16, 6, 196, 196]          0       \n",
      "    Linear-172        [[16, 196, 288]]      [16, 196, 288]        83,232     \n",
      "    Dropout-126       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   Attention-41       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Identity-41       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   LayerNorm-85       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-173        [[16, 196, 288]]     [16, 196, 1152]        332,928    \n",
      "      GELU-41        [[16, 196, 1152]]     [16, 196, 1152]           0       \n",
      "    Dropout-127       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Linear-174       [[16, 196, 1152]]      [16, 196, 288]        332,064    \n",
      "      Mlp-41          [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "     Block-41         [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   LayerNorm-86       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-175        [[16, 196, 288]]      [16, 196, 864]        249,696    \n",
      "    Dropout-128     [[16, 6, 196, 196]]   [16, 6, 196, 196]          0       \n",
      "    Linear-176        [[16, 196, 288]]      [16, 196, 288]        83,232     \n",
      "    Dropout-129       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   Attention-42       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Identity-42       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   LayerNorm-87       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-177        [[16, 196, 288]]     [16, 196, 1152]        332,928    \n",
      "      GELU-42        [[16, 196, 1152]]     [16, 196, 1152]           0       \n",
      "    Dropout-130       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Linear-178       [[16, 196, 1152]]      [16, 196, 288]        332,064    \n",
      "      Mlp-42          [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "     Block-42         [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   LayerNorm-88       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-179        [[16, 196, 288]]      [16, 196, 864]        249,696    \n",
      "    Dropout-131     [[16, 6, 196, 196]]   [16, 6, 196, 196]          0       \n",
      "    Linear-180        [[16, 196, 288]]      [16, 196, 288]        83,232     \n",
      "    Dropout-132       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   Attention-43       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Identity-43       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   LayerNorm-89       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-181        [[16, 196, 288]]     [16, 196, 1152]        332,928    \n",
      "      GELU-43        [[16, 196, 1152]]     [16, 196, 1152]           0       \n",
      "    Dropout-133       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Linear-182       [[16, 196, 1152]]      [16, 196, 288]        332,064    \n",
      "      Mlp-43          [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "     Block-43         [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   LayerNorm-90       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-183        [[16, 196, 288]]      [16, 196, 864]        249,696    \n",
      "    Dropout-134     [[16, 6, 196, 196]]   [16, 6, 196, 196]          0       \n",
      "    Linear-184        [[16, 196, 288]]      [16, 196, 288]        83,232     \n",
      "    Dropout-135       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   Attention-44       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Identity-44       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "   LayerNorm-91       [[16, 196, 288]]      [16, 196, 288]          576      \n",
      "    Linear-185        [[16, 196, 288]]     [16, 196, 1152]        332,928    \n",
      "      GELU-44        [[16, 196, 1152]]     [16, 196, 1152]           0       \n",
      "    Dropout-136       [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "    Linear-186       [[16, 196, 1152]]      [16, 196, 288]        332,064    \n",
      "      Mlp-44          [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "     Block-44         [[16, 196, 288]]      [16, 196, 288]           0       \n",
      "  Transformer-11    [[16, 288, 14, 14]]   [16, 288, 14, 14]          0       \n",
      "     Conv2D-12      [[16, 288, 14, 14]]    [16, 576, 7, 7]         5,760     \n",
      "conv_head_pooling-8 [[16, 288, 14, 14]]    [16, 576, 7, 7]           0       \n",
      "   LayerNorm-92       [[16, 49, 576]]       [16, 49, 576]          1,152     \n",
      "    Linear-188        [[16, 49, 576]]       [16, 49, 1728]        997,056    \n",
      "    Dropout-137      [[16, 12, 49, 49]]    [16, 12, 49, 49]          0       \n",
      "    Linear-189        [[16, 49, 576]]       [16, 49, 576]         332,352    \n",
      "    Dropout-138       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "   Attention-45       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "    Identity-45       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "   LayerNorm-93       [[16, 49, 576]]       [16, 49, 576]          1,152     \n",
      "    Linear-190        [[16, 49, 576]]       [16, 49, 2304]       1,329,408   \n",
      "      GELU-45         [[16, 49, 2304]]      [16, 49, 2304]           0       \n",
      "    Dropout-139       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "    Linear-191        [[16, 49, 2304]]      [16, 49, 576]        1,327,680   \n",
      "      Mlp-45          [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "     Block-45         [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "   LayerNorm-94       [[16, 49, 576]]       [16, 49, 576]          1,152     \n",
      "    Linear-192        [[16, 49, 576]]       [16, 49, 1728]        997,056    \n",
      "    Dropout-140      [[16, 12, 49, 49]]    [16, 12, 49, 49]          0       \n",
      "    Linear-193        [[16, 49, 576]]       [16, 49, 576]         332,352    \n",
      "    Dropout-141       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "   Attention-46       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "    Identity-46       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "   LayerNorm-95       [[16, 49, 576]]       [16, 49, 576]          1,152     \n",
      "    Linear-194        [[16, 49, 576]]       [16, 49, 2304]       1,329,408   \n",
      "      GELU-46         [[16, 49, 2304]]      [16, 49, 2304]           0       \n",
      "    Dropout-142       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "    Linear-195        [[16, 49, 2304]]      [16, 49, 576]        1,327,680   \n",
      "      Mlp-46          [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "     Block-46         [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "   LayerNorm-96       [[16, 49, 576]]       [16, 49, 576]          1,152     \n",
      "    Linear-196        [[16, 49, 576]]       [16, 49, 1728]        997,056    \n",
      "    Dropout-143      [[16, 12, 49, 49]]    [16, 12, 49, 49]          0       \n",
      "    Linear-197        [[16, 49, 576]]       [16, 49, 576]         332,352    \n",
      "    Dropout-144       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "   Attention-47       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "    Identity-47       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "   LayerNorm-97       [[16, 49, 576]]       [16, 49, 576]          1,152     \n",
      "    Linear-198        [[16, 49, 576]]       [16, 49, 2304]       1,329,408   \n",
      "      GELU-47         [[16, 49, 2304]]      [16, 49, 2304]           0       \n",
      "    Dropout-145       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "    Linear-199        [[16, 49, 2304]]      [16, 49, 576]        1,327,680   \n",
      "      Mlp-47          [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "     Block-47         [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "   LayerNorm-98       [[16, 49, 576]]       [16, 49, 576]          1,152     \n",
      "    Linear-200        [[16, 49, 576]]       [16, 49, 1728]        997,056    \n",
      "    Dropout-146      [[16, 12, 49, 49]]    [16, 12, 49, 49]          0       \n",
      "    Linear-201        [[16, 49, 576]]       [16, 49, 576]         332,352    \n",
      "    Dropout-147       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "   Attention-48       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "    Identity-48       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "   LayerNorm-99       [[16, 49, 576]]       [16, 49, 576]          1,152     \n",
      "    Linear-202        [[16, 49, 576]]       [16, 49, 2304]       1,329,408   \n",
      "      GELU-48         [[16, 49, 2304]]      [16, 49, 2304]           0       \n",
      "    Dropout-148       [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "    Linear-203        [[16, 49, 2304]]      [16, 49, 576]        1,327,680   \n",
      "      Mlp-48          [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "     Block-48         [[16, 49, 576]]       [16, 49, 576]            0       \n",
      "  Transformer-12     [[16, 576, 7, 7]]     [16, 576, 7, 7]           0       \n",
      "   MaxPool2D-13      [[16, 576, 7, 7]]     [16, 576, 1, 7]           0       \n",
      "   AvgPool2D-13      [[16, 576, 1, 7]]     [16, 576, 1, 1]           0       \n",
      "   MaxPool2D-14      [[16, 576, 7, 7]]     [16, 576, 7, 1]           0       \n",
      "   AvgPool2D-14      [[16, 576, 7, 1]]     [16, 576, 1, 1]           0       \n",
      "   AvgPool2D-15      [[16, 576, 7, 7]]     [16, 576, 1, 7]           0       \n",
      "   MaxPool2D-15      [[16, 576, 1, 7]]     [16, 576, 1, 1]           0       \n",
      "   AvgPool2D-16      [[16, 576, 7, 7]]     [16, 576, 7, 1]           0       \n",
      "   MaxPool2D-16      [[16, 576, 7, 1]]     [16, 576, 1, 1]           0       \n",
      "   LayerNorm-100        [[16, 2304]]          [16, 2304]           4,608     \n",
      "    Linear-204          [[16, 2304]]           [16, 10]           23,050     \n",
      "===============================================================================\n",
      "Total params: 22,598,074\n",
      "Trainable params: 22,598,074\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------\n",
      "Input size (MB): 9.19\n",
      "Forward/backward pass size (MB): 2352.76\n",
      "Params size (MB): 86.20\n",
      "Estimated Total Size (MB): 2448.15\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "[16, 2304]\n",
      "[16, 10]\n"
     ]
    }
   ],
   "source": [
    "model = pit_s()\r\n",
    "\r\n",
    "paddle.summary(model, (16, 3, 224, 224))\r\n",
    "\r\n",
    "out = model(paddle.randn((16, 3, 224, 224)))\r\n",
    "\r\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10]\n"
     ]
    }
   ],
   "source": [
    "model = pit_s()\r\n",
    "out = model(paddle.randn((1, 3, 224, 224)))\r\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cache file /home/aistudio/.cache/paddle/dataset/cifar/cifar-10-python.tar.gz not found, downloading https://dataset.bj.bcebos.com/cifar/cifar-10-python.tar.gz \n",
      "Begin to download\n",
      "\n",
      "Download finished\n"
     ]
    }
   ],
   "source": [
    "import paddle.vision.transforms as T\r\n",
    "from paddle.vision.datasets import Cifar10\r\n",
    "\r\n",
    "paddle.set_device('gpu')\r\n",
    "\r\n",
    "#数据准备\r\n",
    "transform = T.Compose([\r\n",
    "    T.Resize(size=(224,224)),\r\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],data_format='HWC'),\r\n",
    "    T.ToTensor()\r\n",
    "])\r\n",
    "\r\n",
    "train_dataset = Cifar10(mode='train', transform=transform)\r\n",
    "val_dataset = Cifar10(mode='test',  transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SaveBestModel(paddle.callbacks.Callback):\r\n",
    "    def __init__(self, target=0.5, path='./best_model', verbose=0):\r\n",
    "        self.target = target\r\n",
    "        self.epoch = None\r\n",
    "        self.path = path\r\n",
    "\r\n",
    "    def on_epoch_end(self, epoch, logs=None):\r\n",
    "        self.epoch = epoch\r\n",
    "\r\n",
    "    def on_eval_end(self, logs=None):\r\n",
    "        if logs.get('acc_top1') > self.target:\r\n",
    "            self.target = logs.get('acc_top1')\r\n",
    "            self.model.save(self.path)\r\n",
    "            print('best acc_top1 is {} at epoch {}'.format(self.target, self.epoch+1))\r\n",
    "callback_savebestmodel = SaveBestModel(target=0.5, path='./best_model')\r\n",
    "model = pit_s()\r\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3125/3125 [==============================] - loss: 0.9127 - acc_top1: 0.4418 - acc_top2: 0.6453 - acc_top3: 0.7630 - acc_top4: 0.8376 - acc_top5: 0.8905 - 87ms/step         \n",
      "save checkpoint at /home/aistudio/checkpoint/0\n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.7668 - acc_top1: 0.5390 - acc_top2: 0.7397 - acc_top3: 0.8415 - acc_top4: 0.9024 - acc_top5: 0.9407 - 36ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.539 at epoch 1\n",
      "Epoch 2/50\n",
      "step 3125/3125 [==============================] - loss: 1.3565 - acc_top1: 0.5918 - acc_top2: 0.7879 - acc_top3: 0.8751 - acc_top4: 0.9248 - acc_top5: 0.9554 - 87ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.6867 - acc_top1: 0.6168 - acc_top2: 0.8038 - acc_top3: 0.8912 - acc_top4: 0.9331 - acc_top5: 0.9627 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.6168 at epoch 2\n",
      "Epoch 3/50\n",
      "step 3125/3125 [==============================] - loss: 0.5813 - acc_top1: 0.6566 - acc_top2: 0.8328 - acc_top3: 0.9062 - acc_top4: 0.9470 - acc_top5: 0.9707 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0841 - acc_top1: 0.6405 - acc_top2: 0.8229 - acc_top3: 0.9006 - acc_top4: 0.9475 - acc_top5: 0.9696 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.6405 at epoch 3\n",
      "Epoch 4/50\n",
      "step 3125/3125 [==============================] - loss: 0.9087 - acc_top1: 0.7065 - acc_top2: 0.8662 - acc_top3: 0.9276 - acc_top4: 0.9602 - acc_top5: 0.9793 - 86ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0562 - acc_top1: 0.6427 - acc_top2: 0.8333 - acc_top3: 0.9084 - acc_top4: 0.9496 - acc_top5: 0.9728 - 40ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.6427 at epoch 4\n",
      "Epoch 5/50\n",
      "step 3125/3125 [==============================] - loss: 0.6624 - acc_top1: 0.7486 - acc_top2: 0.8917 - acc_top3: 0.9448 - acc_top4: 0.9713 - acc_top5: 0.9852 - 87ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9192 - acc_top1: 0.6805 - acc_top2: 0.8467 - acc_top3: 0.9174 - acc_top4: 0.9545 - acc_top5: 0.9747 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.6805 at epoch 5\n",
      "Epoch 6/50\n",
      "step 3125/3125 [==============================] - loss: 0.4626 - acc_top1: 0.7854 - acc_top2: 0.9138 - acc_top3: 0.9588 - acc_top4: 0.9796 - acc_top5: 0.9900 - 87ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.5333 - acc_top1: 0.7003 - acc_top2: 0.8609 - acc_top3: 0.9216 - acc_top4: 0.9586 - acc_top5: 0.9770 - 36ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7003 at epoch 6\n",
      "Epoch 7/50\n",
      "step 3125/3125 [==============================] - loss: 0.6119 - acc_top1: 0.8232 - acc_top2: 0.9332 - acc_top3: 0.9695 - acc_top4: 0.9850 - acc_top5: 0.9924 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9942 - acc_top1: 0.7072 - acc_top2: 0.8650 - acc_top3: 0.9263 - acc_top4: 0.9591 - acc_top5: 0.9788 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7072 at epoch 7\n",
      "Epoch 8/50\n",
      "step 3125/3125 [==============================] - loss: 0.9666 - acc_top1: 0.8543 - acc_top2: 0.9500 - acc_top3: 0.9774 - acc_top4: 0.9890 - acc_top5: 0.9952 - 88ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8462 - acc_top1: 0.6724 - acc_top2: 0.8405 - acc_top3: 0.9133 - acc_top4: 0.9502 - acc_top5: 0.9734 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 9/50\n",
      "step 3125/3125 [==============================] - loss: 0.2535 - acc_top1: 0.8804 - acc_top2: 0.9625 - acc_top3: 0.9843 - acc_top4: 0.9934 - acc_top5: 0.9969 - 86ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9728 - acc_top1: 0.7041 - acc_top2: 0.8586 - acc_top3: 0.9225 - acc_top4: 0.9562 - acc_top5: 0.9742 - 36ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 10/50\n",
      "step 3125/3125 [==============================] - loss: 0.3233 - acc_top1: 0.9032 - acc_top2: 0.9723 - acc_top3: 0.9892 - acc_top4: 0.9958 - acc_top5: 0.9983 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0909 - acc_top1: 0.7099 - acc_top2: 0.8614 - acc_top3: 0.9221 - acc_top4: 0.9548 - acc_top5: 0.9754 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7099 at epoch 10\n",
      "Epoch 11/50\n",
      "step 3125/3125 [==============================] - loss: 0.2326 - acc_top1: 0.9221 - acc_top2: 0.9801 - acc_top3: 0.9931 - acc_top4: 0.9972 - acc_top5: 0.9989 - 87ms/step         \n",
      "save checkpoint at /home/aistudio/checkpoint/10\n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0072 - acc_top1: 0.7132 - acc_top2: 0.8703 - acc_top3: 0.9298 - acc_top4: 0.9621 - acc_top5: 0.9797 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7132 at epoch 11\n",
      "Epoch 12/50\n",
      "step 3125/3125 [==============================] - loss: 0.0942 - acc_top1: 0.9362 - acc_top2: 0.9846 - acc_top3: 0.9950 - acc_top4: 0.9983 - acc_top5: 0.9995 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8267 - acc_top1: 0.7189 - acc_top2: 0.8680 - acc_top3: 0.9304 - acc_top4: 0.9616 - acc_top5: 0.9788 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7189 at epoch 12\n",
      "Epoch 13/50\n",
      "step 3125/3125 [==============================] - loss: 0.1172 - acc_top1: 0.9460 - acc_top2: 0.9897 - acc_top3: 0.9970 - acc_top4: 0.9992 - acc_top5: 0.9998 - 88ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.3316 - acc_top1: 0.6994 - acc_top2: 0.8579 - acc_top3: 0.9221 - acc_top4: 0.9550 - acc_top5: 0.9775 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 14/50\n",
      "step 3125/3125 [==============================] - loss: 0.1372 - acc_top1: 0.9548 - acc_top2: 0.9918 - acc_top3: 0.9979 - acc_top4: 0.9993 - acc_top5: 0.9998 - 87ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0143 - acc_top1: 0.7164 - acc_top2: 0.8674 - acc_top3: 0.9265 - acc_top4: 0.9604 - acc_top5: 0.9787 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 15/50\n",
      "step 3125/3125 [==============================] - loss: 0.1132 - acc_top1: 0.9594 - acc_top2: 0.9931 - acc_top3: 0.9983 - acc_top4: 0.9995 - acc_top5: 0.9998 - 87ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.2293 - acc_top1: 0.7202 - acc_top2: 0.8740 - acc_top3: 0.9335 - acc_top4: 0.9635 - acc_top5: 0.9806 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7202 at epoch 15\n",
      "Epoch 16/50\n",
      "step 3125/3125 [==============================] - loss: 0.0434 - acc_top1: 0.9643 - acc_top2: 0.9946 - acc_top3: 0.9987 - acc_top4: 0.9996 - acc_top5: 0.9999 - 87ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.7685 - acc_top1: 0.7289 - acc_top2: 0.8703 - acc_top3: 0.9306 - acc_top4: 0.9610 - acc_top5: 0.9788 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7289 at epoch 16\n",
      "Epoch 17/50\n",
      "step 3125/3125 [==============================] - loss: 0.0807 - acc_top1: 0.9663 - acc_top2: 0.9949 - acc_top3: 0.9988 - acc_top4: 0.9997 - acc_top5: 0.9999 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8609 - acc_top1: 0.7249 - acc_top2: 0.8723 - acc_top3: 0.9318 - acc_top4: 0.9624 - acc_top5: 0.9805 - 36ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 18/50\n",
      "step 3125/3125 [==============================] - loss: 0.2458 - acc_top1: 0.9707 - acc_top2: 0.9958 - acc_top3: 0.9992 - acc_top4: 0.9998 - acc_top5: 0.9999 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.7496 - acc_top1: 0.7310 - acc_top2: 0.8773 - acc_top3: 0.9331 - acc_top4: 0.9634 - acc_top5: 0.9807 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.731 at epoch 18\n",
      "Epoch 19/50\n",
      "step 3125/3125 [==============================] - loss: 0.1342 - acc_top1: 0.9703 - acc_top2: 0.9957 - acc_top3: 0.9989 - acc_top4: 0.9997 - acc_top5: 0.9998 - 89ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0037 - acc_top1: 0.7333 - acc_top2: 0.8730 - acc_top3: 0.9311 - acc_top4: 0.9625 - acc_top5: 0.9798 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7333 at epoch 19\n",
      "Epoch 20/50\n",
      "step 3125/3125 [==============================] - loss: 0.0480 - acc_top1: 0.9779 - acc_top2: 0.9968 - acc_top3: 0.9994 - acc_top4: 0.9998 - acc_top5: 0.9999 - 87ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.2404 - acc_top1: 0.7249 - acc_top2: 0.8732 - acc_top3: 0.9329 - acc_top4: 0.9626 - acc_top5: 0.9800 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 21/50\n",
      "step 3125/3125 [==============================] - loss: 0.1986 - acc_top1: 0.9742 - acc_top2: 0.9971 - acc_top3: 0.9994 - acc_top4: 0.9998 - acc_top5: 1.0000 - 87ms/step        \n",
      "save checkpoint at /home/aistudio/checkpoint/20\n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.2831 - acc_top1: 0.7218 - acc_top2: 0.8682 - acc_top3: 0.9312 - acc_top4: 0.9622 - acc_top5: 0.9784 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 22/50\n",
      "step 3125/3125 [==============================] - loss: 0.0920 - acc_top1: 0.9807 - acc_top2: 0.9978 - acc_top3: 0.9994 - acc_top4: 0.9998 - acc_top5: 1.0000 - 87ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.1643 - acc_top1: 0.7240 - acc_top2: 0.8689 - acc_top3: 0.9299 - acc_top4: 0.9629 - acc_top5: 0.9801 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 23/50\n",
      "step 3125/3125 [==============================] - loss: 0.2529 - acc_top1: 0.9793 - acc_top2: 0.9973 - acc_top3: 0.9995 - acc_top4: 0.9998 - acc_top5: 1.0000 - 86ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8799 - acc_top1: 0.7345 - acc_top2: 0.8763 - acc_top3: 0.9332 - acc_top4: 0.9647 - acc_top5: 0.9802 - 43ms/step        \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7345 at epoch 23\n",
      "Epoch 24/50\n",
      "step 3125/3125 [==============================] - loss: 0.0258 - acc_top1: 0.9797 - acc_top2: 0.9978 - acc_top3: 0.9996 - acc_top4: 0.9998 - acc_top5: 0.9999 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.3355 - acc_top1: 0.7348 - acc_top2: 0.8789 - acc_top3: 0.9362 - acc_top4: 0.9636 - acc_top5: 0.9795 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7348 at epoch 24\n",
      "Epoch 25/50\n",
      "step 3125/3125 [==============================] - loss: 0.0797 - acc_top1: 0.9801 - acc_top2: 0.9972 - acc_top3: 0.9995 - acc_top4: 0.9998 - acc_top5: 0.9999 - 87ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8340 - acc_top1: 0.7309 - acc_top2: 0.8754 - acc_top3: 0.9328 - acc_top4: 0.9607 - acc_top5: 0.9797 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 26/50\n",
      "step 3125/3125 [==============================] - loss: 0.0393 - acc_top1: 0.9827 - acc_top2: 0.9978 - acc_top3: 0.9996 - acc_top4: 1.0000 - acc_top5: 1.0000 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.3104 - acc_top1: 0.7327 - acc_top2: 0.8737 - acc_top3: 0.9309 - acc_top4: 0.9605 - acc_top5: 0.9783 - 36ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 27/50\n",
      "step 3125/3125 [==============================] - loss: 0.0744 - acc_top1: 0.9843 - acc_top2: 0.9981 - acc_top3: 0.9997 - acc_top4: 1.0000 - acc_top5: 1.0000 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.6946 - acc_top1: 0.7382 - acc_top2: 0.8817 - acc_top3: 0.9363 - acc_top4: 0.9639 - acc_top5: 0.9796 - 36ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7382 at epoch 27\n",
      "Epoch 28/50\n",
      "step 3125/3125 [==============================] - loss: 0.0277 - acc_top1: 0.9836 - acc_top2: 0.9979 - acc_top3: 0.9996 - acc_top4: 0.9999 - acc_top5: 1.0000 - 85ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.1296 - acc_top1: 0.7339 - acc_top2: 0.8751 - acc_top3: 0.9321 - acc_top4: 0.9634 - acc_top5: 0.9781 - 36ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 29/50\n",
      "step 3125/3125 [==============================] - loss: 0.0123 - acc_top1: 0.9855 - acc_top2: 0.9984 - acc_top3: 0.9997 - acc_top4: 0.9999 - acc_top5: 1.0000 - 86ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.3702 - acc_top1: 0.7254 - acc_top2: 0.8696 - acc_top3: 0.9300 - acc_top4: 0.9620 - acc_top5: 0.9775 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 30/50\n",
      "step 3125/3125 [==============================] - loss: 0.1245 - acc_top1: 0.9848 - acc_top2: 0.9981 - acc_top3: 0.9996 - acc_top4: 0.9998 - acc_top5: 1.0000 - 86ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0718 - acc_top1: 0.7392 - acc_top2: 0.8741 - acc_top3: 0.9326 - acc_top4: 0.9620 - acc_top5: 0.9783 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7392 at epoch 30\n",
      "Epoch 31/50\n",
      "step 3125/3125 [==============================] - loss: 0.0109 - acc_top1: 0.9843 - acc_top2: 0.9983 - acc_top3: 0.9997 - acc_top4: 0.9999 - acc_top5: 1.0000 - 86ms/step        \n",
      "save checkpoint at /home/aistudio/checkpoint/30\n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.4589 - acc_top1: 0.7400 - acc_top2: 0.8802 - acc_top3: 0.9385 - acc_top4: 0.9640 - acc_top5: 0.9798 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.74 at epoch 31\n",
      "Epoch 32/50\n",
      "step 3125/3125 [==============================] - loss: 0.0964 - acc_top1: 0.9851 - acc_top2: 0.9982 - acc_top3: 0.9997 - acc_top4: 0.9999 - acc_top5: 1.0000 - 88ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.1503 - acc_top1: 0.7215 - acc_top2: 0.8612 - acc_top3: 0.9243 - acc_top4: 0.9545 - acc_top5: 0.9742 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 33/50\n",
      "step 3125/3125 [==============================] - loss: 0.0982 - acc_top1: 0.9866 - acc_top2: 0.9984 - acc_top3: 0.9997 - acc_top4: 1.0000 - acc_top5: 1.0000 - 85ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8342 - acc_top1: 0.7188 - acc_top2: 0.8718 - acc_top3: 0.9307 - acc_top4: 0.9609 - acc_top5: 0.9783 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 34/50\n",
      "step 3125/3125 [==============================] - loss: 0.0472 - acc_top1: 0.9869 - acc_top2: 0.9987 - acc_top3: 0.9998 - acc_top4: 0.9999 - acc_top5: 1.0000 - 87ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9011 - acc_top1: 0.7512 - acc_top2: 0.8875 - acc_top3: 0.9410 - acc_top4: 0.9683 - acc_top5: 0.9814 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "best acc_top1 is 0.7512 at epoch 34\n",
      "Epoch 35/50\n",
      "step 3125/3125 [==============================] - loss: 0.0338 - acc_top1: 0.9856 - acc_top2: 0.9986 - acc_top3: 0.9997 - acc_top4: 0.9999 - acc_top5: 1.0000 - 86ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.3914 - acc_top1: 0.7480 - acc_top2: 0.8879 - acc_top3: 0.9373 - acc_top4: 0.9653 - acc_top5: 0.9803 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 36/50\n",
      "step 3125/3125 [==============================] - loss: 0.0262 - acc_top1: 0.9891 - acc_top2: 0.9990 - acc_top3: 0.9998 - acc_top4: 1.0000 - acc_top5: 1.0000 - 85ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.1593 - acc_top1: 0.7399 - acc_top2: 0.8795 - acc_top3: 0.9331 - acc_top4: 0.9614 - acc_top5: 0.9782 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 37/50\n",
      "step 3125/3125 [==============================] - loss: 0.0258 - acc_top1: 0.9883 - acc_top2: 0.9987 - acc_top3: 0.9998 - acc_top4: 1.0000 - acc_top5: 1.0000 - 87ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.5623 - acc_top1: 0.7337 - acc_top2: 0.8760 - acc_top3: 0.9299 - acc_top4: 0.9622 - acc_top5: 0.9788 - 36ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 38/50\n",
      "step 3125/3125 [==============================] - loss: 0.0217 - acc_top1: 0.9867 - acc_top2: 0.9987 - acc_top3: 0.9997 - acc_top4: 0.9999 - acc_top5: 1.0000 - 88ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.2141 - acc_top1: 0.7398 - acc_top2: 0.8799 - acc_top3: 0.9341 - acc_top4: 0.9652 - acc_top5: 0.9788 - 36ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 39/50\n",
      "step 3125/3125 [==============================] - loss: 0.1099 - acc_top1: 0.9889 - acc_top2: 0.9990 - acc_top3: 0.9998 - acc_top4: 1.0000 - acc_top5: 1.0000 - 85ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.6642 - acc_top1: 0.7447 - acc_top2: 0.8849 - acc_top3: 0.9359 - acc_top4: 0.9631 - acc_top5: 0.9798 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 40/50\n",
      "step 3125/3125 [==============================] - loss: 0.0225 - acc_top1: 0.9885 - acc_top2: 0.9987 - acc_top3: 0.9997 - acc_top4: 0.9999 - acc_top5: 1.0000 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.7958 - acc_top1: 0.7457 - acc_top2: 0.8783 - acc_top3: 0.9336 - acc_top4: 0.9644 - acc_top5: 0.9800 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 41/50\n",
      "step 3125/3125 [==============================] - loss: 0.0081 - acc_top1: 0.9897 - acc_top2: 0.9988 - acc_top3: 0.9998 - acc_top4: 1.0000 - acc_top5: 1.0000 - 87ms/step         \n",
      "save checkpoint at /home/aistudio/checkpoint/40\n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9065 - acc_top1: 0.7450 - acc_top2: 0.8866 - acc_top3: 0.9412 - acc_top4: 0.9656 - acc_top5: 0.9816 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 42/50\n",
      "step 3125/3125 [==============================] - loss: 0.0218 - acc_top1: 0.9894 - acc_top2: 0.9987 - acc_top3: 0.9998 - acc_top4: 0.9999 - acc_top5: 1.0000 - 86ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.4316 - acc_top1: 0.7437 - acc_top2: 0.8782 - acc_top3: 0.9333 - acc_top4: 0.9621 - acc_top5: 0.9796 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 43/50\n",
      "step 3125/3125 [==============================] - loss: 0.0393 - acc_top1: 0.9905 - acc_top2: 0.9988 - acc_top3: 0.9997 - acc_top4: 0.9999 - acc_top5: 1.0000 - 87ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.5619 - acc_top1: 0.7442 - acc_top2: 0.8808 - acc_top3: 0.9330 - acc_top4: 0.9614 - acc_top5: 0.9804 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 44/50\n",
      "step 3125/3125 [==============================] - loss: 0.0065 - acc_top1: 0.9901 - acc_top2: 0.9989 - acc_top3: 0.9999 - acc_top4: 1.0000 - acc_top5: 1.0000 - 87ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0336 - acc_top1: 0.7459 - acc_top2: 0.8885 - acc_top3: 0.9395 - acc_top4: 0.9680 - acc_top5: 0.9817 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 45/50\n",
      "step 3125/3125 [==============================] - loss: 0.0367 - acc_top1: 0.9898 - acc_top2: 0.9989 - acc_top3: 0.9998 - acc_top4: 1.0000 - acc_top5: 1.0000 - 85ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8550 - acc_top1: 0.7352 - acc_top2: 0.8769 - acc_top3: 0.9351 - acc_top4: 0.9633 - acc_top5: 0.9780 - 36ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 46/50\n",
      "step 3125/3125 [==============================] - loss: 0.0301 - acc_top1: 0.9922 - acc_top2: 0.9992 - acc_top3: 0.9999 - acc_top4: 1.0000 - acc_top5: 1.0000 - 87ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9561 - acc_top1: 0.7450 - acc_top2: 0.8834 - acc_top3: 0.9379 - acc_top4: 0.9642 - acc_top5: 0.9807 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 47/50\n",
      "step 3125/3125 [==============================] - loss: 0.0100 - acc_top1: 0.9893 - acc_top2: 0.9990 - acc_top3: 0.9997 - acc_top4: 0.9999 - acc_top5: 1.0000 - 85ms/step         \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.9033 - acc_top1: 0.7477 - acc_top2: 0.8852 - acc_top3: 0.9398 - acc_top4: 0.9673 - acc_top5: 0.9812 - 35ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 48/50\n",
      "step 3125/3125 [==============================] - loss: 0.0203 - acc_top1: 0.9918 - acc_top2: 0.9993 - acc_top3: 0.9999 - acc_top4: 1.0000 - acc_top5: 1.0000 - 88ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.8856 - acc_top1: 0.7156 - acc_top2: 0.8587 - acc_top3: 0.9199 - acc_top4: 0.9506 - acc_top5: 0.9717 - 36ms/step        \n",
      "Eval samples: 10000\n",
      "Epoch 49/50\n",
      "step 3125/3125 [==============================] - loss: 0.0060 - acc_top1: 0.9885 - acc_top2: 0.9989 - acc_top3: 0.9997 - acc_top4: 0.9999 - acc_top5: 1.0000 - 85ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 0.4642 - acc_top1: 0.7490 - acc_top2: 0.8845 - acc_top3: 0.9390 - acc_top4: 0.9641 - acc_top5: 0.9819 - 36ms/step         \n",
      "Eval samples: 10000\n",
      "Epoch 50/50\n",
      "step 3125/3125 [==============================] - loss: 0.0232 - acc_top1: 0.9913 - acc_top2: 0.9990 - acc_top3: 0.9998 - acc_top4: 1.0000 - acc_top5: 1.0000 - 87ms/step        \n",
      "Eval begin...\n",
      "step 625/625 [==============================] - loss: 1.0175 - acc_top1: 0.7478 - acc_top2: 0.8814 - acc_top3: 0.9354 - acc_top4: 0.9627 - acc_top5: 0.9772 - 35ms/step        \n",
      "Eval samples: 10000\n",
      "save checkpoint at /home/aistudio/checkpoint/final\n"
     ]
    }
   ],
   "source": [
    "scheduler = opt.lr.CosineAnnealingDecay(learning_rate=0.000375, T_max=50)\r\n",
    "model.prepare(optimizer=paddle.optimizer.AdamW(weight_decay=0.05,learning_rate=scheduler, parameters=model.parameters()),\r\n",
    "              loss=paddle.nn.CrossEntropyLoss(),\r\n",
    "              metrics=paddle.metric.Accuracy(topk=(1,2,3,4,5)))\r\n",
    "\r\n",
    "visualdl=paddle.callbacks.VisualDL(log_dir='visual_log') # 开启训练可视化\r\n",
    "callbacks=[visualdl,callback_savebestmodel]\r\n",
    "model.fit(\r\n",
    "    train_data=train_dataset, \r\n",
    "    eval_data=val_dataset, \r\n",
    "    batch_size=16, \r\n",
    "    epochs=50,\r\n",
    "    verbose=1,\r\n",
    "    save_dir='checkpoint',\r\n",
    "    save_freq=10,\r\n",
    "    callbacks=callbacks\r\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
